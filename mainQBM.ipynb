{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUANTUM BOLTZMAN MACHINE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Boltzmann Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def energy(z, b, W):\n",
    "    \"\"\" \n",
    "    E(z) = - sum_a (b_a z_a) - sum_{a,b} (w_{a,b} z_a z_b)\n",
    "    for a single configuration z in {+1, -1}^N. \n",
    "    \"\"\"\n",
    "    # We'll compute this directly:\n",
    "    #   E(z) = - (b·z + z^T W z)\n",
    "    bz = np.dot(b, z)\n",
    "    zWz = np.dot(z, np.matmul(W, z))\n",
    "    return -(bz + zWz)\n",
    "\n",
    "def boltzmann_distribution(b, W, all_states):\n",
    "    \"\"\"\n",
    "    Enumerate all states z in {+1, -1}^N, compute\n",
    "        P_model(z) = exp[-E(z)] / Z\n",
    "    and return a (2^N,) probability vector.\n",
    "    \"\"\"\n",
    "    energies = []\n",
    "    for z in all_states:\n",
    "        E_z = energy(z, b, W)\n",
    "        energies.append(E_z)\n",
    "    energies = np.stack(energies)  # shape (2^N)\n",
    "\n",
    "    # exponentiate -E(z) already appears in 'energies' as negative\n",
    "    # but we have E_z = -(...) so actually we want exp(-E_z) = exp(+ ...).\n",
    "    # Let's just do exp(-E_z):\n",
    "    negE = -energies\n",
    "    exp_shifted = np.exp(negE)\n",
    "    Z = exp_shifted.sum()\n",
    "    return exp_shifted / Z\n",
    "\n",
    "# Kullback-Leibler (KL) divergence: KL = Likelyhood - Likelyhood_min\n",
    "def kl_divergence(P_data, P_model):\n",
    "    return np.sum(P_data * np.log((P_data + 1e-12)/(P_model + 1e-12)))\n",
    "\n",
    "# Define Parameters\n",
    "N = 8  # Number of visible spins z in {+1, -1}\n",
    "M = 8  # number of modes\n",
    "p = 0.9 # spin alignment probability with its mode center\n",
    "\n",
    "eta = 0.3  # learning rate\n",
    "num_steps = 35\n",
    "\n",
    "# Generate M random center points s^k in {+1, -1}^N\n",
    "centers = np.random.randint(low=0, high=2, size=(M, N))  # in {0,1}\n",
    "centers = 2*centers - 1  # map to {+1,-1}\n",
    "\n",
    "# Enumerate all states z in {+1, -1}^N\n",
    "all_configs = list(itertools.product([-1, +1], repeat=N))\n",
    "all_z = np.array(all_configs, dtype=np.float32)  # (2^N, N)\n",
    "\n",
    "def mixture_data_distribution(all_states, centers, p):  \n",
    "    num_modes = centers.shape[0]  # The number of modes (M=8) is the centers' number of rows\n",
    "    N_ = centers.shape[1]         # The number of bits (N=10) is the centers' number of columns \n",
    "    N_states = all_states.shape[0]  # (2^N)\n",
    "    probs = np.zeros(N_states, dtype=np.float32) \n",
    "    for s in range(N_states): \n",
    "        mode_sum = 0.0\n",
    "        for k in range(num_modes):\n",
    "            d_ks = 0.5 * np.sum(1 - all_states[s, :] * centers[k, :])  # Hamming distance between state s and center k\n",
    "            mode_sum += p**(N_ - d_ks) * (1 - p)**d_ks \n",
    "        probs[s] = mode_sum / num_modes  # Generating P_data for each state\n",
    "    # normalitation\n",
    "    probs /= probs.sum()\n",
    "    return probs\n",
    "\n",
    "P_data = mixture_data_distribution(all_z, centers, p)\n",
    "print(\"Check sum of P_data:\", P_data.sum().item())  # ~1.0\n",
    "print(\"Check dimension of P_data:\", P_data.shape)  # ~2^10 = 1024\n",
    "\n",
    "# Compute \"positive phase\" averages once: <z_a>_data, <z_a z_b>_data for each a,b = 1,2,...,N\n",
    "z_data_avg = np.zeros(N)\n",
    "zz_data_avg = np.zeros((N, N))\n",
    "N_states = all_z.shape[0]\n",
    "for i in range(N_states):\n",
    "    z_data_avg += P_data[i] * all_z[i, :] \n",
    "    zz_data_avg += P_data[i] * np.outer(all_z[i, :], all_z[i, :])\n",
    "\n",
    "# Manual Gradient Updates Using exact formulas\n",
    "# Initialize parameters (b, W) using 'random.seed'\n",
    "np.random.seed(42)\n",
    "b = 0.01 * np.random.randn(N)\n",
    "W = 0.01 * np.random.randn(N, N)\n",
    "\n",
    "kl_history = []\n",
    "for step in range(num_steps):\n",
    "    # 1) Compute model distribution\n",
    "    P_model = boltzmann_distribution(b, W, all_z)\n",
    "    # 2) 'Negative phase' averages: <z_a>_model, <z_a z_b>_model for each a,b = 1,2,...,N\n",
    "    z_model_avg = np.zeros(N)\n",
    "    zz_model_avg = np.zeros((N, N))\n",
    "    for i in range(N_states):\n",
    "      z_model_avg += P_model[i] * all_z[i, :]\n",
    "      zz_model_avg += P_model[i] * np.outer(all_z[i, :], all_z[i, :])\n",
    "    \n",
    "    # Compute gradient steps as difference between positive and negative phases\n",
    "    db = eta * (z_data_avg - z_model_avg) \n",
    "    dW = eta * (zz_data_avg - zz_model_avg)\n",
    "\n",
    "    b += db\n",
    "    W += dW\n",
    "    \n",
    "    # Compute and save KL value\n",
    "    this_kl = kl_divergence(P_data, P_model)\n",
    "    kl_history.append(this_kl.item())\n",
    "\n",
    "    if step % 5 == 0:\n",
    "        print(f\"Iter {step}: KL = {this_kl.item():.4f}\")\n",
    "\n",
    "# Saving Data frame in CSV\n",
    "df = pd.DataFrame({\"iteration\": list(range(num_steps)), \"kl_history\": kl_history})\n",
    "df.to_csv(\"BM.csv\", index=False)\n",
    "print(\"Dati salvati in BM.csv\")\n",
    "\n",
    "df = pd.read_csv(\"BM.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting results of BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(df['iteration'], df['kl_history'], marker='o', label='KL Divergence')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"KL Divergence\")\n",
    "plt.title(\"BM Training (Exact)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bound - Quantum Boltzmann Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import expm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Define Parameters\n",
    "N = 8  # Number of visible qubits\n",
    "M = 8  # Number of modes for data distribution\n",
    "p = 0.9  # Probability of alignment\n",
    "eta = 0.8 # Learning rate (increased)\n",
    "iterations = 35  # Number of optimization steps\n",
    "Gamma = 2 # Fixed transverse field strength\n",
    "\n",
    "# Pauli Matrices\n",
    "I = np.array([[1, 0], [0, 1]])\n",
    "sigma_z = np.array([[1, 0], [0, -1]])\n",
    "sigma_x = np.array([[0, 1], [1, 0]])\n",
    "\n",
    "# Generate M random center points s^k in {+1, -1}^N\n",
    "centers = np.random.randint(low=0, high=2, size=(M, N)) # in {0,1}\n",
    "centers = 2*centers - 1  # map to {+1,-1}\n",
    "\n",
    "def mixture_data_distribution(all_states, centers, p):  \n",
    "    num_modes = centers.shape[0]  # The number of modes (M=8) is the centers' number of rows\n",
    "    N_ = centers.shape[1]         # The number of bits (N=10) is the centers' number of columns \n",
    "    N_states = all_states.shape[0] # (2^N)\n",
    "    probs = np.zeros(N_states, dtype=np.float32)\n",
    "    for s in range(N_states):  \n",
    "        mode_sum = 0.0\n",
    "        for k in range(num_modes): \n",
    "            d_ks = 0.5 * np.sum(1 - all_states[s, :] * centers[k, :])  # Hamming distance between state s and center k\n",
    "            mode_sum += p**(N_ - d_ks) * (1 - p)**d_ks \n",
    "        probs[s] = mode_sum / num_modes   # Generating P_data for each state\n",
    "    # normalitation\n",
    "    probs /= probs.sum()\n",
    "    return probs\n",
    "\n",
    "def tensor_product(ops):\n",
    "    \"\"\"Compute the tensor product of multiple operators.\"\"\"\n",
    "    result = ops[0]\n",
    "    for op in ops[1:]:\n",
    "        result = np.kron(result, op)\n",
    "    return result\n",
    "\n",
    "# Compute sigma_z(a), sigma_x(a) and sigma_z(a,b) matrices for each a,b = 1,...,N\n",
    "b_sigma = np.zeros(N, dtype=object)\n",
    "gamma_sigma = np.zeros((2**N, 2**N))\n",
    "W_sigma = np.zeros((N, N),dtype=object)\n",
    "for a in range(N):\n",
    "    gamma_sigma += tensor_product([I] * a + [sigma_x] + [I] * (N - a - 1))\n",
    "    b_sigma[a] = tensor_product([I] * a + [sigma_z] + [I] * (N - a - 1)) \n",
    "    for b in range(a + 1, N): \n",
    "        W_sigma[a, b] = tensor_product([I] * a + [sigma_z] + [I] * (b - a - 1) + [sigma_z] + [I] * (N - b - 1))\n",
    "\n",
    "def build_states(N):\n",
    "    all_states = np.zeros((2**N, N))\n",
    "    for s in range(N):\n",
    "        all_states[:, s] = np.diag(b_sigma[s])  # each state is a diagonal element of the sigma_z(a) matrices \n",
    "    return all_states\n",
    "\n",
    "def build_hamiltonian(N, Gamma, b, W):\n",
    "    \"\"\"Construct the Fully Visible QBM Hamiltonian with a transverse field.\"\"\"\n",
    "    H = np.zeros((2**N, 2**N), dtype=complex) # Size (2^N, 2^N)\n",
    "    H = -Gamma * gamma_sigma  # Transverse field\n",
    "    H -= np.dot(b, b_sigma)  \n",
    "    H -= np.sum(W * W_sigma, axis=None)  \n",
    "    return H\n",
    "\n",
    "def compute_density_matrix(H):\n",
    "    \"\"\"Compute the density matrix rho = exp(-H) / Z.\"\"\"\n",
    "    exp_H = expm(-H)\n",
    "    Z = np.trace(exp_H)\n",
    "    rho = exp_H / Z\n",
    "    return rho, Z\n",
    "\n",
    "def compute_full_probability_distribution(rho):\n",
    "    \"\"\"Compute the full probability distribution P_v from diagonal elements of rho.\"\"\"\n",
    "    return np.real(np.diag(rho))  # Extract diagonal elements as probabilities\n",
    "\n",
    "# Kullback-Leibler (KL) divergence: KL = Likelyhood - Likelyhood_min\n",
    "def compute_kl_upper_bound(P_data, P_model):\n",
    "    \"\"\"Compute the KL divergence upper bound using P_model: diagonal elements of rho.\"\"\"\n",
    "    return np.sum(P_data * np.log((P_data + 1e-12)/(P_model + 1e-12)))\n",
    "\n",
    "# Compute \"positive\" and \"negative phase\" averages: <sigma_z_a>, <sigma_z_a sigma_z_b> for each a,b = 1,2,...,N\n",
    "def compute_gradient_update(P_data, rho, all_states, N, eta):\n",
    "    \"\"\"Compute gradient updates for b and w.\"\"\"\n",
    "\n",
    "    z_model_avg = np.zeros(N)\n",
    "    zz_model_avg = np.zeros((N, N))\n",
    "    z_data_avg = np.zeros(N)\n",
    "    zz_data_avg = np.zeros((N, N))\n",
    "\n",
    "    # Negative phase\n",
    "    N_states = all_states.shape[0]\n",
    "    for a in range(N):\n",
    "      z_model_avg[a] = np.trace(rho @ b_sigma[a]).real\n",
    "      for b in range(a + 1, N):\n",
    "         zz_model_avg[a, b] = np.trace(rho @ W_sigma[a, b]).real\n",
    "         #zz_model_avg[b, a] = zz_model_avg[a, b]\n",
    "    \n",
    "    # Positive phase\n",
    "    N_states = all_states.shape[0]\n",
    "    for i in range(N_states):\n",
    "        z_data_avg += P_data[i] * all_states[i, :]\n",
    "        zz_data_avg += P_data[i] * np.outer(all_states[i, :], all_states[i, :])\n",
    "\n",
    "    # Compute gradient steps as difference between positive and negative phases\n",
    "    delta_b = eta * (z_data_avg - z_model_avg)\n",
    "    delta_W = eta * (zz_data_avg - zz_model_avg)\n",
    "    return delta_b, delta_W\n",
    "\n",
    "def optimize_qbm(P_data, all_states, N, Gamma, b, W, eta, iterations):\n",
    "    \"\"\"Optimize the Fully Visible Bound-Based QBM.\"\"\"\n",
    "\n",
    "    kl_upper_bounds = []\n",
    "    for it in range(iterations):\n",
    "        H = build_hamiltonian(N, Gamma, b, W)\n",
    "        rho, _ = compute_density_matrix(H)\n",
    "\n",
    "        # Compute model distribution\n",
    "        P_model = compute_full_probability_distribution(rho)\n",
    "        \n",
    "        # Compute and save KL value\n",
    "        KL_bound = compute_kl_upper_bound(P_data, P_model)\n",
    "        kl_upper_bounds.append(KL_bound)\n",
    "        \n",
    "        delta_b, delta_W = compute_gradient_update(P_data, rho, all_states, N, eta)\n",
    "        b += delta_b\n",
    "        W += delta_W\n",
    "\n",
    "        print(f\"Iteration {it+1}/{iterations}, KL Upper Bound: {KL_bound:.6f}, Δb={np.linalg.norm(delta_b):.6f}, Δw={np.linalg.norm(delta_W):.6f}\")\n",
    "    return kl_upper_bounds\n",
    "\n",
    "# Initialize parameters (b, W) using 'random.seed'\n",
    "np.random.seed(42)\n",
    "b = 0.01 * np.random.randn(N)\n",
    "W = 0.01 * np.random.randn(N, N)\n",
    "\n",
    "all_states = build_states(N)\n",
    "P_data = mixture_data_distribution(all_states, centers, p)\n",
    "print(\"Check sum of P_data:\", P_data.sum().item())  # ~1.0\n",
    "print(\"Check dimension of P_data:\", P_data.shape)  # ~2^10 = 1024\n",
    "print(type(P_data))\n",
    "\n",
    "# Optimize the Fully Visible Bound-Based QBM\n",
    "kl_upper_bounds = optimize_qbm(P_data, all_states, N, Gamma, b, W, eta, iterations)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"iteration\": range(1, iterations + 1), \"kl_upper_bounds\": kl_upper_bounds})\n",
    "\n",
    "# Saving Data frame in CSV\n",
    "df.to_csv(\"FullyVisible_bQBM.csv\", index=False)\n",
    "print(\"Dati salvati in FullyVisible_bQBM.csv\")\n",
    "\n",
    "df = pd.read_csv(\"FullyVisible_bQBM.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting results of b-QBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot KL divergence upper bound over iterations\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(df['iteration'], df['kl_upper_bounds'], marker='o', label='KL Upper Bound over Iterations')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"KL Upper Bound\")\n",
    "plt.title(\"KL Upper Bound Over Iterations (FV-bQBM)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Boltzmann Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import expm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Define Parameters\n",
    "N = 10  # Number of visible qubits\n",
    "M = 8  # Number of modes for data distribution\n",
    "p = 0.9  # Probability of alignment\n",
    "eta = 0.4 # Learning rate (increased)\n",
    "iterations = 35  # Number of optimization steps\n",
    "\n",
    "# Pauli Matrices\n",
    "I = np.array([[1, 0], [0, 1]])\n",
    "sigma_z = np.array([[1, 0], [0, -1]])\n",
    "sigma_x = np.array([[0, 1], [1, 0]])\n",
    "\n",
    "# Generate M random center points s^k in {+1, -1}^N\n",
    "centers = np.random.randint(low=0, high=2, size=(M, N)) # in {0,1}\n",
    "centers = 2*centers - 1  # map to {+1,-1}\n",
    "\n",
    "def mixture_data_distribution(all_states, centers, p):  \n",
    "    num_modes = centers.shape[0]  # The number of modes (M=8) is the centers' number of rows\n",
    "    N_ = centers.shape[1]         # The number of bits (N=10) is the centers' number of columns \n",
    "    N_states = all_states.shape[0] # (2^N)\n",
    "    probs = np.zeros(N_states, dtype=np.float32)\n",
    "    for s in range(N_states):  \n",
    "        mode_sum = 0.0\n",
    "        for k in range(num_modes): \n",
    "            d_ks = 0.5 * np.sum(1 - all_states[s, :] * centers[k, :])  # Hamming distance between state s and center k\n",
    "            mode_sum += p**(N_ - d_ks) * (1 - p)**d_ks \n",
    "        probs[s] = mode_sum / num_modes   # Generating P_data for each state\n",
    "    # normalitation\n",
    "    probs /= probs.sum()\n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tensor_product(ops):\n",
    "    \"\"\"Compute the tensor product of multiple operators.\"\"\"\n",
    "    result = ops[0]\n",
    "    for op in ops[1:]:\n",
    "        result = np.kron(result, op)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We report that while trying to speed-up the code we also tried to compute the function 'tensor_product' using NUMBA. Unfortunately this didn't seem to make the computation faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "\n",
    "@njit\n",
    "def tensor_product(ops):\n",
    "    \"\"\"Compute the tensor product of multiple operators.\"\"\"\n",
    "    result = ops[0]\n",
    "    for op in ops[1:]:\n",
    "        result = np.kron(result, op)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to optimize the efficiency of the code we computed the (2^N x 2^N) matrices, representing spin operators, only once out of the for loops. This did make the run time faster: about 5 times faster than creating spin matrices in each cycle.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sigma_z(a), sigma_x(a) and sigma_z(a,b) matrices for each a,b = 1,...,N\n",
    "gamma_sigma = np.zeros((2**N, 2**N))\n",
    "b_sigma = np.zeros(N, dtype=object)\n",
    "W_sigma = np.zeros((N, N),dtype=object)\n",
    "for a in range(N):\n",
    "    gamma_sigma += tensor_product([I] * a + [sigma_x] + [I] * (N - a - 1))\n",
    "    b_sigma[a] = tensor_product([I] * a + [sigma_z] + [I] * (N - a - 1)) \n",
    "    for b in range(a + 1, N): \n",
    "        W_sigma[a,b] = tensor_product([I] * a + [sigma_z] + [I] * (b - a - 1) + [sigma_z] + [I] * (N - b - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_states(N):\n",
    "    all_states = np.zeros((2**N, N))\n",
    "    for s in range(N):\n",
    "        all_states[:, s] = np.diag(b_sigma[s])  # each state is a diagonal element of the sigma_z(a) matrices \n",
    "    return all_states\n",
    "\n",
    "def build_hamiltonian(N, Gamma, b, W):\n",
    "    \"\"\"Construct the Fully Visible QBM Hamiltonian with a transverse field.\"\"\"\n",
    "    H = np.zeros((2**N, 2**N), dtype=complex) # Size (2^N, 2^N)\n",
    "    H = -Gamma * gamma_sigma  # Transverse field\n",
    "    H -= np.dot(b, b_sigma) \n",
    "    H -= np.sum(W * W_sigma, axis=None) \n",
    "    return H\n",
    "\n",
    "def compute_density_matrix(H):\n",
    "    \"\"\"Compute the density matrix rho = e^(-H)/Z\"\"\"\n",
    "    exp_H = expm(-H)  # exp(-H)\n",
    "    Z = np.trace(exp_H)\n",
    "    return exp_H / Z, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again trying to speed up the computation, we computed the exponential of the matrix H approximatevely. Even with a very few order of approsimization, this did make the code a little faster, but still didn't allow computation for bigger N. Therefore, we ended up using the numpy function 'expm()' in the final results. \n",
    "\n",
    "Note: We tried using Numba for this function as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximation of matrix exponentiation\n",
    "def matrix_exponential_approx(A, n_terms=2):\n",
    "    result = np.eye(A.shape[0])\n",
    "    term = np.eye(A.shape[0]) \n",
    "    for i in range(1, n_terms + 1):\n",
    "        term = np.dot(term, A) / i  \n",
    "        result += term  \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_full_probability_distribution(rho):\n",
    "    \"\"\"Return the diagonal elements of rho as the model probability distribution.\"\"\"\n",
    "    return np.real(np.diag(rho))\n",
    "\n",
    "# Kullback-Leibler (KL) divergence: KL = Likelyhood - Likelyhood_min\n",
    "def compute_kl_upper_bound(P_data, P_model):\n",
    "    \"\"\"Compute the KL divergence upper bound using P_model: diagonal elements of rho.\"\"\"\n",
    "    return np.sum(P_data * np.log((P_data + 1e-12)/(P_model + 1e-12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building derivative to compute \"positive phase\"\n",
    "def compute_partial_expH(H, rho, Z, projector, partial_H, n):\n",
    "   avg_v = 0.0\n",
    "   delta_t = 1.0 / n\n",
    "   trace = np.trace(projector @ (rho * Z))\n",
    "   exp_tH = expm(-delta_t * H)\n",
    "   exp1 = np.eye(H.shape[0])\n",
    "   for m in range(1, n + 1):\n",
    "      t = m * delta_t\n",
    "      exp1 = exp1 @ exp_tH  # e^(-τH)\n",
    "      exp2 = expm((t - 1) * H) # e^{-(1-τ)H}\n",
    "      avg_v += np.trace(projector @ exp1 @ partial_H @ exp2) / (trace + 1e-12) * delta_t\n",
    "   return -avg_v\n",
    "\n",
    "# Compute \"positive\" and \"negative phase\" averages: <sigma_z_a>, <sigma_z_a sigma_z_b> for each a,b = 1,2,...,N\n",
    "\n",
    "state_proj = np.zeros((2**N, 2**N))\n",
    "\n",
    "def compute_gradient_update(P_data, H, rho, Z, all_states, N, eta):\n",
    "    \"\"\"Compute the gradient updates for the QBM parameters.\"\"\"\n",
    "    n = 2\n",
    "    global state_proj\n",
    "\n",
    "    z_model_avg = np.zeros(N)\n",
    "    zz_model_avg = np.zeros((N, N))\n",
    "    z_data_avg = np.zeros(N)\n",
    "    zz_data_avg = np.zeros((N, N)) \n",
    "\n",
    "    N_states = all_states.shape[0]\n",
    "    partial_expH_Gamma = np.zeros(N_states)\n",
    "    for z in range(N_states):\n",
    "        state_proj[z, z] = 1\n",
    "        partial_expH_Gamma[z] = compute_partial_expH(H, rho, Z, state_proj, gamma_sigma, n)\n",
    "        state_proj[z, z] = 0\n",
    "\n",
    "    for a in range(N):\n",
    "      z_model_avg[a] = np.trace(rho @ b_sigma[a]).real\n",
    "      Gamma_data_avg = 0.\n",
    "      for z in range(N_states):\n",
    "            state_proj[z, z] = 1\n",
    "            z_data_avg[a] += P_data[z] * compute_partial_expH(H, rho, Z, state_proj, b_sigma[a], n)\n",
    "            Gamma_data_avg += P_data[z] * partial_expH_Gamma[z]\n",
    "            state_proj[z, z] = 0\n",
    "      for b in range(a + 1, N):\n",
    "         zz_model_avg[a, b] = np.trace(rho @  W_sigma[a,b]).real\n",
    "         #zz_model_avg[b, a] = zz_model_avg[a, b]  # Ensure symmetry\n",
    "         for z in range(N_states):\n",
    "            state_proj[z, z] = 1\n",
    "            zz_data_avg[a, b] += P_data[z] * compute_partial_expH(H, rho, Z, state_proj,  W_sigma[a, b], n)\n",
    "            #zz_data_avg[b, a] = zz_data_avg[a, b]  \n",
    "            state_proj[z, z] = 0\n",
    "             \n",
    "    Gamma_model_avg = np.trace(rho @ gamma_sigma).real\n",
    "\n",
    "    # Compute gradient steps as difference between positive and negative phases\n",
    "    delta_b = -eta * (z_data_avg + z_model_avg)\n",
    "    delta_W = -eta * (zz_data_avg + zz_model_avg)\n",
    "    delta_Gamma = -eta * (Gamma_data_avg + Gamma_model_avg)\n",
    "\n",
    "    return delta_b, delta_W, delta_Gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our last attempt to speed-up the computation, to be able to compute the machine learning algorithm for bigger number of qubits N, was to substitute the function 'compute_partial_expH' with a Fortran function that computed the same derivative. Analizying the code we noticed that this one was the computationally most demanding function and removing it would allow the algorithm to run for N=10. Indeed this function was the main and only difference fron the b-QBM algorithm which did run for bigger N (N = 10).\n",
    "We tried first implementing with Fortran a test function that only computed a trace of a matrix (H) to verify the speed-up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fortran test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subroutine test(A, N, avg)\n",
    "    integer, intent(in) :: N\n",
    "    real, dimension(1024, 1024), intent(in) :: A\n",
    "    real, intent(out) :: avg\n",
    "    integer :: i\n",
    "\n",
    "    avg=0.\n",
    "    do i = 1, 2**N\n",
    "      avg = avg + A(i,i)\n",
    "    end do\n",
    "end subroutine test\n",
    "\n",
    "#Note: 'python3 -m numpy.f2py -c -m  test  test.f90' to compile the Fortran f2py module to be able to import it in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/filepath')\n",
    "import test\n",
    "\n",
    "state_proj = np.zeros((2**N, 2**N))\n",
    "\n",
    "def compute_gradient_update(P_data, H, rho, Z, all_states, N, eta):\n",
    "    \"\"\"Compute the gradient updates for the QBM parameters.\"\"\"\n",
    "    n = 2\n",
    "    global state_proj\n",
    "\n",
    "    z_model_avg = np.zeros(N)\n",
    "    zz_model_avg = np.zeros((N, N))\n",
    "    z_data_avg = np.zeros(N)\n",
    "    zz_data_avg = np.zeros((N, N)) \n",
    "\n",
    "    N_states = all_states.shape[0]\n",
    "    partial_expH_Gamma = np.zeros(N_states)\n",
    "    for z in range(N_states):\n",
    "        state_proj[z, z] = 1\n",
    "        partial_expH_Gamma[z] = test.test(H, N)\n",
    "        state_proj[z, z] = 0\n",
    "\n",
    "    for a in range(N):\n",
    "      z_model_avg[a] = np.trace(rho @ b_sigma[a]).real\n",
    "      Gamma_data_avg = 0.\n",
    "      for z in range(N_states):\n",
    "            state_proj[z, z] = 1\n",
    "            z_data_avg[a] += P_data[z] * test.test(H, N)\n",
    "            Gamma_data_avg += P_data[z] * partial_expH_Gamma[z]\n",
    "            state_proj[z, z] = 0\n",
    "      for b in range(a + 1, N):\n",
    "         zz_model_avg[a, b] = np.trace(rho @  W_sigma[a,b]).real\n",
    "         #zz_model_avg[b, a] = zz_model_avg[a, b]  # Ensure symmetry\n",
    "         for z in range(N_states):\n",
    "            state_proj[z, z] = 1\n",
    "            zz_data_avg[a, b] += P_data[z] * test.test(H, N)\n",
    "            #zz_data_avg[b, a] = zz_data_avg[a, b]  \n",
    "            state_proj[z, z] = 0\n",
    "             \n",
    "    Gamma_model_avg = np.trace(rho @ gamma_sigma).real\n",
    "\n",
    "    # Compute gradient steps as difference between positive and negative phases\n",
    "    delta_b = -eta * (z_data_avg + z_model_avg)\n",
    "    delta_W = -eta * (zz_data_avg + zz_model_avg)\n",
    "    delta_Gamma = -eta * (Gamma_data_avg + Gamma_model_avg)\n",
    "\n",
    "    return delta_b, delta_W, delta_Gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then computed the same test function using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(H):\n",
    "    avg = np.trace(H)\n",
    "    return avg\n",
    "\n",
    "state_proj = np.zeros((2**N, 2**N))\n",
    "\n",
    "def compute_gradient_update(P_data, H, rho, Z, all_states, N, eta):\n",
    "    \"\"\"Compute the gradient updates for the QBM parameters.\"\"\"\n",
    "    n = 2\n",
    "    global state_proj\n",
    "\n",
    "    z_model_avg = np.zeros(N)\n",
    "    zz_model_avg = np.zeros((N, N))\n",
    "    z_data_avg = np.zeros(N)\n",
    "    zz_data_avg = np.zeros((N, N)) \n",
    "\n",
    "    N_states = all_states.shape[0]\n",
    "    partial_expH_Gamma = np.zeros(N_states)\n",
    "    for z in range(N_states):\n",
    "        state_proj[z, z] = 1\n",
    "        partial_expH_Gamma[z] = test(H)\n",
    "        state_proj[z, z] = 0\n",
    "\n",
    "    for a in range(N):\n",
    "      z_model_avg[a] = np.trace(rho @ b_sigma[a]).real\n",
    "      Gamma_data_avg = 0.\n",
    "      for z in range(N_states):\n",
    "            state_proj[z, z] = 1\n",
    "            z_data_avg[a] += P_data[z] * test(H)\n",
    "            Gamma_data_avg += P_data[z] * partial_expH_Gamma[z]\n",
    "            state_proj[z, z] = 0\n",
    "      for b in range(a + 1, N):\n",
    "         zz_model_avg[a, b] = np.trace(rho @  W_sigma[a,b]).real\n",
    "         #zz_model_avg[b, a] = zz_model_avg[a, b]  # Ensure symmetry\n",
    "         for z in range(N_states):\n",
    "            state_proj[z, z] = 1\n",
    "            zz_data_avg[a, b] += P_data[z] * test(H)\n",
    "            #zz_data_avg[b, a] = zz_data_avg[a, b]  \n",
    "            state_proj[z, z] = 0\n",
    "             \n",
    "    Gamma_model_avg = np.trace(rho @ gamma_sigma).real\n",
    "\n",
    "    # Compute gradient steps as difference between positive and negative phases\n",
    "    delta_b = -eta * (z_data_avg + z_model_avg)\n",
    "    delta_W = -eta * (zz_data_avg + zz_model_avg)\n",
    "    delta_Gamma = -eta * (Gamma_data_avg + Gamma_model_avg)\n",
    "\n",
    "    return delta_b, delta_W, delta_Gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that the run-time was slower with the Fortran function. This led us to abandon the implementation with Fortran.\n",
    "\n",
    "![Python](immagine.png)\n",
    "![Fortran](immagine.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_qbm(P_data, all_states, N, Gamma, b, W, eta, iterations):\n",
    "    \"\"\"Optimize the Fully Visible Bound-Based QBM.\"\"\"\n",
    "\n",
    "    kl_divergence = []\n",
    "    for it in range(iterations):\n",
    "        H = build_hamiltonian(N, Gamma, b, W)\n",
    "        rho,_ = compute_density_matrix(H)\n",
    "        _,Z = compute_density_matrix(H)\n",
    "\n",
    "        # Compute model distribution\n",
    "        P_model = compute_full_probability_distribution(rho)\n",
    "\n",
    "        # Compute and save KL value\n",
    "        KL_bound = compute_kl_upper_bound(P_data, P_model)\n",
    "        kl_divergence.append(KL_bound)\n",
    "        \n",
    "        delta_b, delta_W, delta_Gamma = compute_gradient_update(P_data, H, rho, Z, all_states, N, eta)\n",
    "        b += delta_b\n",
    "        W += delta_W\n",
    "        Gamma += delta_Gamma\n",
    "\n",
    "        print(f\"Iteration {it+1}/{iterations}, KL Divergence: {KL_bound:.6f}, Δb={np.linalg.norm(delta_b):.6f}, Δw={np.linalg.norm(delta_W):.6f}\")\n",
    "    return kl_divergence\n",
    "\n",
    "# Initialize parameters (b, W, Gamma) using 'random.seed'\n",
    "np.random.seed(42)\n",
    "b = 0.1 * np.random.randn(N)\n",
    "W = 0.1 * np.random.randn(N, N)\n",
    "Gamma = 0.1 * np.random.rand()\n",
    "\n",
    "all_states = build_states(N)\n",
    "P_data = mixture_data_distribution(all_states, centers, p)\n",
    "print(\"Check sum of P_data:\", P_data.sum().item())  # ~1.0\n",
    "print(\"Check dimension of P_data:\", P_data.shape)  # ~2^10 = 1024\n",
    "print(type(P_data))\n",
    "\n",
    "# Optimize the Fully Visible QBM\n",
    "kl_divergence = optimize_qbm(P_data, all_states, N, Gamma, b, W, eta, iterations)\n",
    "\n",
    "df = pd.DataFrame({\"iteration\": range(1, iterations + 1), \"kl_divergence\": kl_divergence})\n",
    "\n",
    "# Saving Data frame in CSV\n",
    "df.to_csv(\"FullyVisible_QBM.csv\", index=False)\n",
    "print(\"Dati salvati in FullyVisible_QBM.csv\")\n",
    "\n",
    "df = pd.read_csv(\"FullyVisible_QBM.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting results of QBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot KL divergence upper bound over iterations\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(df['iteration'], df['kl_divergence'], marker='o', label='KL divergence over Iterations')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"KL divergence\")\n",
    "plt.title(\"KL divergence Over Iterations (FV-QBM)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting results of BM, b-QBM and QBM all together for a comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file_path = \"/Users/andreadecristofaro/BM.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "plt.plot(df['iteration'], df['kl_history'], marker='o', color='green', label='BM_N=7')\n",
    "\n",
    "file_path = \"/Users/andreadecristofaro/FullyVisible_bQBM.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "plt.plot(df['iteration'], df['kl_upper_bounds'], marker='o', color='red', label='bQBM_N=7')\n",
    "\n",
    "file_path = \"/Users/andreadecristofaro/FullyVisible_QBM.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "plt.plot(df['iteration'], df['kl_divergence'], marker='o', color='blue', label='QBM_N=7')\n",
    "\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"KL\")\n",
    "plt.title(\"KL divergence Over Iterations\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
